input {
  tcp {
    port => 50000
    codec => json
    add_field  => { "log_source" => "django" }
  }
  file {
    path => "/var/log/postgres/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    # codec => line
    add_field  => { "log_source" => "postgres-log" }
  }
  file {
    path => "/var/log/redis/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    # codec => line
    add_field  => { "log_source" => "redis-log" }
  }
  file {
    path => "/var/log/vault/*.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    add_field  => { "log_source" => "vault-log" }
  }
  file {
    path => "/var/log/nginx/access.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    add_field  => { "log_source" => "nginx-log" }
  }
  file {
    path => "/var/log/modlog/audit.log"  # Replace with the actual path to your log files
    start_position => "beginning"
    codec => json
    add_field  => { "log_source" => "modsec-log" }
  }
}

filter {
  if [log_source] == "django" {
    # Lowercase the level field and rename it to match the standard 'level' in Elasticsearch
    mutate {
      lowercase => ["levelname"]  # Convert 'levelname' to lowercase
      rename => { "levelname" => "level" }  # Rename for consistent index field name
    }

    # Parse the timestamp from Django to ensure it's used as @timestamp in Elasticsearch
    date {
      match => ["asctime", "YYYY-MM-dd HH:mm:ss"]
      target => "@timestamp"
      remove_field => ["asctime"]  # Clean up to avoid duplicate timestamp fields
    }
  }
  if [log_source] == "postgres-log" {
    # Parse the timestamp, PID, log level, and message using grok
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:timezone} \[%{NUMBER:pid}\] %{WORD:log_level}: %{GREEDYDATA:log_message}"
      }
    }
    # Optionally, parse the timestamp into a standard format for Elasticsearch
    date {
      match => ["timestamp", "YYYY-MM-dd HH:mm:ss.SSS"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
    # Optional: Add any other mutations or field renaming as necessary
    mutate {
      # Rename fields if necessary, e.g., rename 'log_level' to 'severity'
      rename => { "log_level" => "severity" }
    }
  }
  if [log_source] == "vault-log" {
    grok {
      match => {
        "message" => '^{"@level":"%{WORD:log_level}","@message":"(?<msg>[^"]*)","@module":"(?<module>[^"]*)","@timestamp":"(?<timestamp>[^"]*)".*$'
      }
    }

    # Convert timestamp to @timestamp field in Elasticsearch
    date {
      match => ["timestamp", "ISO8601"]
      remove_field => ["timestamp"]
    }

    # Rename fields for clarity
    mutate {
      rename => { "msg" => "message" }
      remove_field => ["namespace"]
    }
  }
  if [log_source] == "redis-log" {
    grok {
      match => { "message" => "%{NUMBER:pid}:%{WORD:role} %{DATE_US:date} %{TIME:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:message}" }
    }

    date {
      match => ["date", "dd MMM yyyy"]
      target => "@timestamp"
    }
  }
  if [log_source] == "nginx-log" {
    grok {
      match => {
        "message" => '\{ "level":"%{WORD:level}", "ts": "%{TIMESTAMP_ISO8601:timestamp}", "message": "handled request %{WORD:request_method} %{URIPATH:request_uri}", "request": \{ "id": "%{DATA:http_x_request_id}", "remote_ip": "%{IPV4:remote_ip}", "remote_port": "%{NUMBER:remote_port}", "protocol": "%{DATA:server_protocol}", "method": "%{WORD:request_method}", "host": "%{DATA:host}", "uri": "%{DATA:request_uri}", "headers": \{ "user-agent": "%{DATA:http_user_agent}", "accept": "%{DATA:http_accept}", "accept-encoding": "%{DATA:http_accept_encoding}", "traceparent": "%{DATA:http_traceparent}", "tracestate": "%{DATA:http_tracestate}" \} \}, "bytes_read": %{NUMBER:request_length}, "duration_msecs": %{NUMBER:request_time}, "size": %{NUMBER:bytes_sent}, "status": %{NUMBER:status}, "resp_headers": \{ "content_length": "%{DATA:sent_http_content_length}", "content_type": "%{DATA:sent_http_content_type}" \} \}'
      }
    }
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"  # Sets the parsed timestamp to @timestamp field
    }
  }
if [log_source] == "modsec-log" {
    # First parse the JSON structure
    json {
      source => "message"
      target => "[modsec]"
      remove_field => ["message"]
    }

    # Extract nested transaction fields
    mutate {
      rename => {
        "[modsec][transaction][client_ip]" => "client_ip"
        "[modsec][transaction][time_stamp]" => "timestamp"
        "[modsec][transaction][unique_id]" => "unique_id"
        "[modsec][transaction][request][method]" => "http_method"
        "[modsec][transaction][response][http_code]" => "http_status"
        "[modsec][messages][0][message]" => "alert_message"
        "[modsec][messages][0][details][ruleId]" => "rule_id"
      }
    }

    # Parse the custom timestamp format
    date {
      match => ["timestamp", "EEE MMM dd HH:mm:ss yyyy"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }

    # Optional: Clean up remaining nested fields
    mutate {
      remove_field => [
        "[modsec][transaction][host_ip]",
        "[modsec][transaction][client_port]",
        "[modsec][transaction][host_port]",
        "[modsec][producer]"
      ]
    }
  }
  mutate {
    remove_field => ["type"]
  }
}

output {
  if [log_source] == "django" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "django-logs-%{+YYYY.MM.dd}"  # Adjusted to only include date in the index
      user => "elastic"  # Use environment variables for security
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  if [log_source] == "postgres-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "postgres-logs-%{+YYYY.MM.dd}"  # New index for PostgreSQL logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  if [log_source] == "vault-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "vault-logs-%{+YYYY.MM.dd}"  # New index for vault logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  if [log_source] == "redis-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "redis-logs-%{+YYYY.MM.dd}"  # New index for redis logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  if [log_source] == "nginx-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "nginx-logs-%{+YYYY.MM.dd}"  # New index for redis logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  if [log_source] == "modsec-log" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "modsec-logs-%{+YYYY.MM.dd}"  # New index for redis logs
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      # data_stream => "auto"
    }
  }
  # stdout {
  #   codec => rubydebug
  # }
}